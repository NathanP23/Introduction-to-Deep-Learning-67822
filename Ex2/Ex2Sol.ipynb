{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf47033",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning 67822 - [Ex2](https://docs.google.com/document/d/1HdHzN-z-TDoVNHxM_3NR1WWvFvxM1H6sm1HyncPvcCA/edit?pli=1&tab=t.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64532e0e",
   "metadata": {},
   "source": [
    "## Programming Tasks\n",
    "Classifying and encoding the MNISTdigit dataset.\n",
    "\n",
    "The MNIST dataset consists of 60,000 (+10,000 test) small images of scanned hand-written digits (0-9). The dataset contains the digits values as labels. The original images are 28-by-28 pixels. The images are monochromatic, i.e., have a single brightness channel with values between zero (black) and one (white).\n",
    "\n",
    "In this exercise we will design and train both autoencoding and classification CNN networks.\n",
    "\n",
    "Specific tasks:\n",
    "For every section that includes training, paste the code of the training loop at the end of the section. If there are inner functions, you donâ€™t need to paste them, as long they have clear, informative names. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7536cde",
   "metadata": {},
   "source": [
    "### 1\n",
    "**Autoencoder**. Define a convolutional autoencoder to encode (and decode) the images through a small dimensional latent space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ec6c6",
   "metadata": {},
   "source": [
    "#### a\n",
    "How will you make the architecture flexible enough to choose any latent dim d, regardless of the number of channels chosen? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d330e5f",
   "metadata": {},
   "source": [
    "#### b\n",
    "Explore two network configurations, one with a small number of channels (around 4 in the first layer, and similar numbers in the following layers) and a larger one (around 16 in the first layer) and report these tests as well; input vs. reconstructed images and the reconstruction loss obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823ba18",
   "metadata": {},
   "source": [
    "#### c\n",
    "Explore the loss you get for d=4/16, and rationalize it in your report. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9bd1f",
   "metadata": {},
   "source": [
    "#### d\n",
    "Describe and explain the network architecture you choose for this particular data (stride factors / #layers / #filters in each layer / non-linearity used). \n",
    "\n",
    "The best practice of implementing this code is by defining separate encoder and decoder models, both inheriting from nn.module. Use a mean L1 error to define the reconstruction loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649d621",
   "metadata": {},
   "source": [
    "### 2\n",
    "**Classifier**. Use the same architecture as the encoder in Q1 in order to define a classification network by combining it with a single-layered MLP network that will map the latent space into a 10 (digits) class prediction. Train this network to predict the digit classes using cross-entropy loss in two scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e6965",
   "metadata": {},
   "source": [
    "#### (i) \n",
    "over the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e9211",
   "metadata": {},
   "source": [
    "#### (ii)\n",
    "over only 100 random training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb81f74",
   "metadata": {},
   "source": [
    "#### Plot the training and test errors as well as accuracies. \n",
    "The best practice here is to keep the model divided into two models: the original encoder model and the added MLP model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98acbba",
   "metadata": {},
   "source": [
    "### 3\n",
    "**Pre-trained Representation**. Repeat the two tests in Q2, this time by using the unsupervised pre-trained encoder weights from Q1 as a fixed (non-trainable encoder model), and only train the final MLP, once with the entire dataset, once with only 100 random training examples. Plot the training and test errors as well as accuracies. How do they compare to the ones in Q2? Write your conclusion as to the usefulness of unsupervised representation learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5147a7f",
   "metadata": {},
   "source": [
    "### 4\n",
    "**Task Specific Encoding**: both Q1 and Q2 produce two different trained encoding networks. The one in Q1 already has a matching trained decoder, but the one in Q2 does not. Use this pretrained encoding network as a fixed (non-trainable model) and train a matching decoder over the entire dataset. Meaning that you are using an encoder trained for classification task, and coupling it with a decoder trained for reconstruction (trained from scratch along with the pretrained encoding/classifying network from Q2). Look at reconstructed images produced by these two sets of encoder-decoder networks and answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2a8c8",
   "metadata": {},
   "source": [
    "#### a\n",
    "Which one results in better reconstruction error and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708dfd5",
   "metadata": {},
   "source": [
    "#### b\n",
    "Describe the qualitative differences between the reconstructed results they produce. Explain why you think these are the differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e2675",
   "metadata": {},
   "source": [
    "#### c\n",
    "Where do you see higher in-class (per-digit) variability? (watch and show multiple instances of the same digit to answer this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e752ea4",
   "metadata": {},
   "source": [
    "#### d\n",
    "Where do you see higher inter-class (between digits) distance/separation? Explain both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a8142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements saved to requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze > requirements.txt\n",
    "echo \"Requirements saved to requirements.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_Ex2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
