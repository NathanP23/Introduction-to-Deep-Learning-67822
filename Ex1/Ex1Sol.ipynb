{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6166850e",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning 67822 - [Ex1](https://docs.google.com/document/d/11Q1ejfwTH_tHjdQob0gYLA3bS88lNsBStpBWz085rB0/edit?tab=t.0)\n",
    "#### NAME1 (ID1) & NAME2 (ID2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd986d4c",
   "metadata": {},
   "source": [
    "##### Section 1: Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8db7d1",
   "metadata": {},
   "source": [
    "###### Task 1: Split training data (from the .txt files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bb7e2",
   "metadata": {},
   "source": [
    "We are training a model to classify 9-mer peptides based on whether they are detected by the immune system via specific HLA alleles. Each positive sample is associated with one of six common alleles. The negative samples are peptides not detected by any of the alleles.\n",
    "\n",
    "When splitting the data into training and test sets, it’s crucial to avoid introducing bias. One tempting idea is to take the first 90% of each file for training and the last 10% for testing. However, this assumes that the peptide order inside each file is random — which may not be true. The files might be sorted by binding strength, similarity, or even alphabetically, which could skew the distribution.\n",
    "\n",
    "To prevent such biases and ensure fair training and evaluation, we use a **stratified random split per allele**:\n",
    "\n",
    "1. We load and shuffle the peptides from each positive allele file individually.\n",
    "2. We split each file into a 90% training / 10% test set.\n",
    "3. We do the same for the negative examples (from `negs.txt`).\n",
    "4. Finally, we combine all subsets and shuffle them again.\n",
    "\n",
    "This approach ensures that all alleles are represented in both training and test sets, the overall class balance between positive and negative is maintained and no ordering bias from the original files leaks into the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531f7145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 33642(89.99%)\n",
      "Pos: 11600(34.48%), Neg: 22042(65.52%)\n",
      "Test set size:  3741(10.01%)\n",
      "Pos: 1291(34.51%), Neg: 2450(65.49%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Config\n",
    "data_dir = Path(\"Data/HLA_Dataset\")\n",
    "train_ratio = 0.9\n",
    "\n",
    "# Locate all allele-positive files\n",
    "allele_files = [f for f in data_dir.glob(\"*.txt\") if \"neg\" not in f.name]\n",
    "\n",
    "# Store train/test samples\n",
    "pos_train, pos_test = [], []\n",
    "\n",
    "# Process each positive file (1 per allele)\n",
    "for file in allele_files:\n",
    "    allele = file.stem.replace(\"_pos\", \"\")\n",
    "    with open(file) as f:\n",
    "        peptides = [line.strip() for line in f if line.strip()]\n",
    "        random.shuffle(peptides)  # shuffle within each allele\n",
    "        split_idx = int(len(peptides) * train_ratio)\n",
    "        pos_train += [(pep, allele, 1) for pep in peptides[:split_idx]]\n",
    "        pos_test  += [(pep, allele, 1) for pep in peptides[split_idx:]]\n",
    "\n",
    "# Process negatives\n",
    "with open(data_dir / \"negs.txt\") as f:\n",
    "    neg_peptides = [line.strip() for line in f if line.strip()]\n",
    "    random.shuffle(neg_peptides)\n",
    "    split_idx = int(len(neg_peptides) * train_ratio)\n",
    "    neg_train = [(pep, \"NEG\", 0) for pep in neg_peptides[:split_idx]]\n",
    "    neg_test  = [(pep, \"NEG\", 0) for pep in neg_peptides[split_idx:]]\n",
    "\n",
    "# Final datasets\n",
    "train_data = pos_train + neg_train\n",
    "test_data = pos_test + neg_test\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "# Summary\n",
    "train_pct = (len(train_data) / (len(train_data) + len(test_data))) * 100\n",
    "test_pct = (len(test_data) / (len(train_data) + len(test_data))) * 100\n",
    "neg_train_pct = (len(neg_train) / len(train_data)) * 100\n",
    "pos_train_pct = (len(pos_train) / len(train_data)) * 100\n",
    "neg_test_pct = (len(neg_test) / len(test_data)) * 100\n",
    "pos_test_pct = (len(pos_test) / len(test_data)) * 100\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}({train_pct:.2f}%)\\nPos: {len(pos_train)}({pos_train_pct:.2f}%), Neg: {len(neg_train)}({neg_train_pct:.2f}%)\")\n",
    "print(f\"Test set size:  {len(test_data)}({test_pct:.2f}%)\\nPos: {len(pos_test)}({pos_test_pct:.2f}%), Neg: {len(neg_test)}({neg_test_pct:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
